{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abbc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision?\n",
    "   If so, how can you go about doing it? If not, what is the reason ?\n",
    "    \n",
    "Ans: Hybrid Model: A technique that combines two or more different machine learning models in some way.\n",
    "                   But we can't get 95 percent precision as all other models give different precision rate, accuracy is differed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3128aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What's the difference between hard voting classifiers and soft voting classifiers ?\n",
    "\n",
    "Ans: In hard voting (also known as majority voting), every individual classifier votes for a class, and the majority wins. \n",
    "     In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels.\n",
    "\n",
    "    In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. \n",
    "    The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options ?\n",
    "\n",
    "Ans: When sampling is performed without replacement, it is called pasting. \n",
    "    In other words, both approaches are similar.\n",
    "    In both cases you are sampling the training data to build multiple instances of a classifier.\n",
    "    Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.\n",
    "    This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. \n",
    "    It is the best starting point for understanding boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is the advantage of evaluating out of the bag ?\n",
    "\n",
    "Ans: The advantage of the OOB method is that it requires less computation and allows one to test the model as it is being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f034c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What distinguishes Extra-Trees from ordinary Random Forests? \n",
    "   What good would this extra randomness do? \n",
    "    Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests ?\n",
    "    \n",
    "Ans: Random forest uses bootstrap replicas, that is to say, it subsamples the input data with replacement, whereas Extra Trees use the whole original sample. \n",
    "     This may increase variance because bootstrapping makes it more diversified.\n",
    "\n",
    "    Random forest adds additional randomness to the model, while growing the trees. \n",
    "    Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. \n",
    "    This results in a wide diversity that generally results in a better model. \n",
    "     Extra Trees is much faster.\n",
    "\n",
    "    This is because instead of looking for the optimal split at each node it does it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data ?\n",
    "\n",
    "Ans: If  AdaBoost ensemble underfits the training data, we can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. \n",
    "      we may also try slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db366ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set ?\n",
    "\n",
    "Ans: If  Gradient Boosting ensemble overfits the training set, we should try decreasing the learning rate.\n",
    "    we  could also use early stopping to find the right number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7bdaff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
