{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44217fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? \n",
    "   What are the major disadvantages ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dimensionality reduction brings many advantages to your machine learning data, including:\n",
    "\n",
    "Fewer features mean less complexity\n",
    "You will need less storage space because you have fewer data\n",
    "Fewer features require less computation time\n",
    "Model accuracy improves due to less misleading data\n",
    "Algorithms train faster thanks to fewer data\n",
    "Reducing the data set’s feature dimensions helps visualize the data faster\n",
    "It removes noise and redundant features\n",
    "\n",
    "\n",
    "Disadvantages Of Dimensionality Reduction\n",
    "We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work.\n",
    "It may need a lot of processing power.\n",
    "Interpreting transformed characteristics might be challenging.\n",
    "The independent variables become harder to comprehend as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is the dimensionality curse ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "If many dimensions reside in the feature space, that results in a large volume of space. \n",
    "Consequently, the points in the space and rows of data may represent only a tiny, non-representative sample. \n",
    "This imbalance can negatively affect machine learning algorithm performance. \n",
    "This condition is known as “the curse of dimensionality.”  \n",
    "A data set with vast input features complicates the predictive modeling task, putting performance and accuracy at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? \n",
    "If so, how can you go about doing it? If not, what is the reason ?\n",
    "\n",
    "\n",
    "Ans: No, dimensionality reduction is not reversible in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b451fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables ?\n",
    "\n",
    "\n",
    "Ans:   Yes, PCA can be used to significantly reduce dimensionality of highly non-linear dataset because it can at least get rid of useless dimensions.\n",
    "       If there are no useless dimensions, reducing dimensionality with PCA will lose too much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72aa8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. \n",
    "  What is the number of dimensions that the resulting dataset would have ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63238c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "I perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%.\n",
    "In this case roughly 950 dimensions are required to preserve 95% of the variance. \n",
    "So the answer is, it depends on the dataset, and it could be any number between 1 and 950."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "The following are the scenarios where the following are used:\n",
    "\n",
    "Vanilla PCA: the dataset fit in memory\n",
    "Incremental PCA: larget dataset that don't fit in memory, online taks\n",
    "Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
    "kernel PCA: used for nonlinear PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. How do you assess a dimensionality reduction algorithm's success on your dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78096be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.Is it logical to use two different dimensionality reduction algorithms in a chain ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "It often make any sense to chain two different dimensionality reduction algorithms.\n",
    "A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
